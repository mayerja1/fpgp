{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate plots\n",
    "Run this notebook to generate used plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotting_tools\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp, names = [utils.LoadedLogs('data/f2_1e7_evals_coev_8predsz')], ['coev, size 8']\n",
    "plotting_tools.compare_performance(exp, 'evals', 'test_set_f', method_names=names, ignore_tresh=30, xlabel='Evaluations', ylabel='Test set fitness')\n",
    "plt.savefig('plots/f2_1e7_fitness_over_evals_sz8.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = ('data/f2_1e7_evals_exact', \n",
    "               'data/f2_1e7_evals_coev_32predsz', 'data/f2_1e7_evals_coev_64predsz')\n",
    "exp, names = list(map(utils.LoadedLogs, data_paths)), ['exact', 'coev, size 32', 'coev, size 64']\n",
    "plotting_tools.compare_performance(exp, 'evals', 'test_set_f', method_names=names, xlabel='Evaluations', ylabel='Test set fitness')\n",
    "plt.savefig('plots/f2_1e7_fitness_over_evals.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(data_paths[0] + '/dataset.npz')\n",
    "trn_x, trn_y = data['trn_x'], data['trn_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex=True, sharey=True)\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "plt.grid(False)\n",
    "plt.ylabel('Number of runs')\n",
    "plt.xlabel('Test set fitness at the end of the run')\n",
    "ax[0].set_title('coev size 8')\n",
    "ax[1].set_title('exact')\n",
    "ax[2].set_title('coev size 32')\n",
    "ax[3].set_title('coev size 64')\n",
    "ax[0].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_1e7_evals_coev_8predsz')], bins=20, range=(0, 15))\n",
    "ax[1].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_1e7_evals_exact')], range=(0, 15), bins=20)\n",
    "ax[2].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_1e7_evals_coev_32predsz')], range=(0, 15), bins=20)\n",
    "ax[3].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_1e7_evals_coev_64predsz')], range=(0, 15), bins=20)\n",
    "plt.savefig('plots/f2_1e7_fitness_hist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = ('data/f2_3.5e7_evals_exact', 'data/f2_3.5e7_evals_coev_32predsz')\n",
    "exp, names = list(map(utils.LoadedLogs, data_paths)), ['exact', 'coev, size 32']\n",
    "plotting_tools.compare_performance(exp, 'evals', 'test_set_f', method_names=names, xlabel='Evaluations', ylabel='Test set fitness')\n",
    "plt.savefig('plots/f2_3.5e7_fitness_over_evals.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = list(list(exp)[1])\n",
    "sorted_logs = sorted(runs, key=lambda x: x[-1]['test_set_f'])\n",
    "log = sorted_logs[1]\n",
    "plotting_tools.predictor_histogram(trn_x, trn_y, log)\n",
    "plt.savefig('plots/f2_predictor_hist.pdf')\n",
    "del sorted_logs\n",
    "del runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "plt.grid(False)\n",
    "plt.ylabel('Number of runs')\n",
    "plt.xlabel('Test set fitness at the end of the run')\n",
    "ax[0].set_title('coev')\n",
    "ax[1].set_title('exact')\n",
    "ax[0].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_3.5e7_evals_coev_32predsz')], bins=20)\n",
    "ax[1].hist([r[-1]['test_set_f'] for r in utils.LoadedLogs('data/f2_3.5e7_evals_exact')], bins=20)\n",
    "plt.savefig('plots/f2_3.5e7_fitness_hist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trn_x, trn_y)\n",
    "plt.savefig('plots/f2_target.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in utils.LoadedLogs('data/f2_1e7_evals_coev_8predsz'):\n",
    "    log = l\n",
    "    break\n",
    "fig, ax = plt.subplots()\n",
    "solutions_vals = log.select('best_sol_vals')\n",
    "predictors = log.select('predictor')\n",
    "test_set_f = log.select('test_set_f')\n",
    "evals = log.select('evals')\n",
    "\n",
    "pred_plot, = plt.plot([], [], ls=' ', marker='o', label='used predictor')\n",
    "target_plot, = plt.plot([], [], color='blue', alpha=0.5, label='target function')\n",
    "sol_plot, = plt.plot([], [], color='red', label='best solution')\n",
    "ax.set_xlim(min(trn_x), max(trn_x))\n",
    "ax.set_ylim(min(trn_y), max(trn_y))\n",
    "target_plot.set_data(trn_x, trn_y)\n",
    "s = '{:.2e}'.format(evals[-1])\n",
    "ax.set_title(f'generation: {len(log)}, evals: {s}\\ntest set fitness: {test_set_f[-1]}')\n",
    "pred_plot.set_data(trn_x[predictors[-1]], trn_y[predictors[-1]])\n",
    "sol_plot.set_data(trn_x, solutions_vals[-1])\n",
    "ax.legend()\n",
    "plt.savefig('plots/f2_failed_run.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
